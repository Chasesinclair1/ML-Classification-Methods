---
title: "3654 Assignment 5"
author: "Chase Sinclair"
date: "4/8/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rattle)
data.wine = wine
```
HONOR CODE: “I have neither given nor received unauthorized assistance on this assignment.” - Chase



# Problem 1
```{r}
rm(list=ls())
library("MASS")
library("ggplot2")
library("gridExtra")
library(rattle)
data.wine = wine
```

This wine dataset comes from the UCI Machine Learning Repository and contains the results of a chemical analysis of wines grown in Italy. The dataset includes 3 different types of wine which are tested under 13 chemical analyses. The dataset is moderatly in size with 178 rows of data (observations). 


#### (A)  Perform classification using LDA (linear discriminant analysis) and report the classification error rate. 
```{r}
lda1 = lda(Type ~., data=data.wine)
pred.lda1 = predict(lda1,data.wine)
table(pred.lda1$class,data.wine$Type)
sum(data.wine$Type!=pred.lda1$class)/nrow(data.wine)
```

From the table we can see that 0 wines are being mis-classified. Thus, the overall error rate is 0%. Note that this is not cross-validation error. This means this is a very good model.

#### (B) Perform classification using QDA (quadratic discriminant analysis) and report the classification error rate.
```{r}
qda1 = qda(Type ~., data=data.wine)
pred.qda1 = predict(qda1,data.wine)
table(pred.qda1$class,data.wine$Type)
sum(data.wine$Type!=pred.qda1$class)/nrow(data.wine)
```

This QDA model has a very low classification error rate as well with an error rate of 0.56%. This means that not even 1 wine will be missclassified.

#### (C)  Perform classification using SVM (support vector machines) and report the classification error rate.
```{r}
library(e1071)
svm1 = svm(Type~.,data=data.wine)
pred.svm1 = predict(svm1,data.wine)
table(pred.svm1,data.wine$Type) 
sum(data.wine$Type!=pred.svm1)/nrow(data.wine)
```
This SVM model has a very low classification error rate as well with an error rate of 0%. This means that no wine will be missclassified. This is obviously a very good model.

#### (D)Rank the classification methods in your order of preference for this dataset, and justify your ranking. 

The best classification methods after running the models will be LDA and SVM because both give us a 0% error rate, then QDA will therefore be rank 2 because it has an error rate above 0%. However, all models are very good in general. 


#### Cross-validation Error Table 70%
```{r}
n = nrow(data.wine)
train.prop = 0.7
train.size = ceiling(n*train.prop)     # "ceiling" syntax: https://www.tutorialgateway.org/r-ceiling-function/
test.size = n-train.size

K = 100
for (i in 1:K){
  foo = sample(n,train.size)
  train.data = data.wine[foo,]
  test.data = data.wine[-foo,]
  lda.cv = lda(Type~.,data=train.data)
  qda.cv = qda(Type~.,data=train.data)
  svm.cv = svm(Type~.,data=train.data)
  
  pred.lda = predict(lda.cv, test.data)
  pred.qda = predict(qda.cv, test.data)
  pred.svm = predict(svm.cv, test.data)
  
  lda.err.cv = sum(test.data$Type!=pred.lda$class)/nrow(test.data)
  qda.err.cv = sum(test.data$Type!=pred.qda$class)/nrow(test.data)
  svm.err.cv = sum(test.data$Type!=pred.svm)/nrow(test.data)
}
  round(mean(lda.err.cv),3);round(mean(qda.err.cv),3);round(mean(svm.err.cv),3)
```

```{r}
n = nrow(data.wine)
train.prop = 0.8
train.size = ceiling(n*train.prop)     # "ceiling" syntax: https://www.tutorialgateway.org/r-ceiling-function/
test.size = n-train.size

K = 100
for (i in 1:K){
  foo = sample(n,train.size)
  train.data = data.wine[foo,]
  test.data = data.wine[-foo,]
  lda.cv = lda(Type~.,data=train.data)
  qda.cv = qda(Type~.,data=train.data)
  svm.cv = svm(Type~.,data=train.data)
  
  pred.lda = predict(lda.cv, test.data)
  pred.qda = predict(qda.cv, test.data)
  pred.svm = predict(svm.cv, test.data)
  
  lda.err.cv[i] = sum(test.data$Type!=pred.lda$class)/nrow(test.data)
  qda.err.cv[i] = sum(test.data$Type!=pred.qda$class)/nrow(test.data)
  svm.err.cv[i] = sum(test.data$Type!=pred.svm)/nrow(test.data)
}
round(mean(lda.err.cv),3);round(mean(qda.err.cv),3);round(mean(svm.err.cv),3)
```

```{r}
n = nrow(data.wine)
train.prop = 0.9
train.size = ceiling(n*train.prop)     # "ceiling" syntax: https://www.tutorialgateway.org/r-ceiling-function/
test.size = n-train.size

K = 100
for (i in 1:K){
  foo = sample(n,train.size)
  train.data = data.wine[foo,]
  test.data = data.wine[-foo,]
  lda.cv = lda(Type~.,data=train.data)
  qda.cv = qda(Type~.,data=train.data)
  svm.cv = svm(Type~.,data=train.data)
  
  pred.lda = predict(lda.cv, test.data)
  pred.qda = predict(qda.cv, test.data)
  pred.svm = predict(svm.cv, test.data)
  
  lda.err.cv[i] = sum(test.data$Type!=pred.lda$class)/nrow(test.data)
  qda.err.cv[i] = sum(test.data$Type!=pred.qda$class)/nrow(test.data)
  svm.err.cv[i] = sum(test.data$Type!=pred.svm)/nrow(test.data)
}
round(mean(lda.err.cv),3);round(mean(qda.err.cv),3);round(mean(svm.err.cv),3)
```

(Not exactly same as output above due to randomness)
```{r}
new <- matrix(c(0,0.015,0.014,0.019,0.012,0.005,0,0.016,0.016),ncol=3,byrow=TRUE)
colnames(new) <- c("Training = 70%","Training = 80%","Training = 90%")
rownames(new) <- c("LDA","QDA","SVM")
smoke <- as.table(new)
new
```

We observe that while the cross-validation errors are small for all three methods, LDA performs the best (lowest average error), followed by SVM, and then QDA.













